{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import botometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  date              user_id           user_name  \\\n",
       "0                                                                 \n",
       "0  2020-10-12 03:48:35  1220170588267958272          TedMazerDS   \n",
       "1  2020-10-12 03:48:35            587957065   Stop the Insanity   \n",
       "2  2020-10-12 03:48:35   763481578689425409           funny gal   \n",
       "3  2020-10-12 03:48:34            874258472  Jonathan Greenberg   \n",
       "4  2020-10-12 03:48:34  1293187865820540928      conservative_Q   \n",
       "\n",
       "              tweet_id                                              tweet  \n",
       "0                                                                          \n",
       "0  1315499580344197121  @GadSaad @JoeBiden Most of the country simply ...  \n",
       "1  1315499579400425477  @Blue_Texas2020 @BryanRobinAR Bush won‚Äôt endor...  \n",
       "2  1315499579027206149  RT @JYSexton: It‚Äôs kind of amazing to watch pe...  \n",
       "3  1315499576082661381  @JimCarrey Why the Barrett confirm rush?\\r\\n \\...  \n",
       "4  1315499575587860480  @DavidJHarrisJr You know it‚Äôs extremely hard t...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>user_id</th>\n      <th>user_name</th>\n      <th>tweet_id</th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>1220170588267958272</td>\n      <td>TedMazerDS</td>\n      <td>1315499580344197121</td>\n      <td>@GadSaad @JoeBiden Most of the country simply ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>587957065</td>\n      <td>Stop the Insanity</td>\n      <td>1315499579400425477</td>\n      <td>@Blue_Texas2020 @BryanRobinAR Bush won‚Äôt endor...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>763481578689425409</td>\n      <td>funny gal</td>\n      <td>1315499579027206149</td>\n      <td>RT @JYSexton: It‚Äôs kind of amazing to watch pe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-10-12 03:48:34</td>\n      <td>874258472</td>\n      <td>Jonathan Greenberg</td>\n      <td>1315499576082661381</td>\n      <td>@JimCarrey Why the Barrett confirm rush?\\r\\n \\...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-10-12 03:48:34</td>\n      <td>1293187865820540928</td>\n      <td>conservative_Q</td>\n      <td>1315499575587860480</td>\n      <td>@DavidJHarrisJr You know it‚Äôs extremely hard t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "df = pd.read_csv('10k_political_tweets.csv', header=None, index_col=0)\n",
    "column_labels = ['date', 'user_id', 'user_name', 'tweet_id', 'tweet']\n",
    "df.columns = column_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import PunktSentenceTokenizer as punkt \n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\daily\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\daily\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "source": [
    "generate the tags for each token in the text, and then lemmatize each word using the tag"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "source": [
    "### Removing noise\n",
    "\n",
    "* links\n",
    "* handles\n",
    "* special chars"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "source": [
    "## Word Density"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_pos = FreqDist(all_pos_words)"
   ]
  },
  {
   "source": [
    "## Data Prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is: 0.9966666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n"
   ]
  },
  {
   "source": [
    "Call on tweets within DF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "tweet_in = \"Crying happy tears of joy rn! My republican, christian dad told me over dinner he was planning to vote for Biden\"\n",
    "\n",
    "tokens_in = remove_noise(word_tokenize(tweet_in))\n",
    "print(classifier.classify(dict([token, True] for token in tokens_in)))"
   ]
  },
  {
   "source": [
    "Loop through df "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7c947fa4343b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# Standard word tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m-> 1241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \"\"\"\n\u001b[1;32m-> 1291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \"\"\"\n\u001b[1;32m-> 1291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1281\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \"\"\"\n\u001b[0;32m   1321\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \"\"\"\n\u001b[0;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "for row in df:\n",
    "    tweet = df['tweet']\n",
    "    token = remove_noise(word_tokenize(tweet))\n",
    "    ans = classifier.classify(dict([token, True] for token in tokens))\n",
    "    df['Sentiment'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for val in df['tweet']:\n",
    "    token = remove_noise(word_tokenize(val))\n",
    "    #ans = classifier.classify(dict([token, True] for token in tokens))\n",
    "    sentiment.append(classifier.classify(dict([token, True] for token in tokens)))\n",
    "\n",
    "df['Sentiment'] = sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for val in df['tweet']:\n",
    "    tokens_in = remove_noise(word_tokenize(val))\n",
    "    sentiment.append(classifier.classify(dict([token, True] for token in tokens_in)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pos:  5842\nNeg:  4158\n"
     ]
    }
   ],
   "source": [
    "countPos = 0 \n",
    "countNeg = 0\n",
    "\n",
    "for val in df['Sentiment']:\n",
    "    if val == 'Positive':\n",
    "        countPos += 1\n",
    "    else:\n",
    "        countNeg += 1\n",
    "print('Pos: ', countPos)\n",
    "print('Neg: ', countNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   date              user_id                 user_name  \\\n",
       "0                                                                        \n",
       "0   2020-10-12 03:48:35  1220170588267958272  TedMazerDS                 \n",
       "1   2020-10-12 03:48:35  587957065            Stop the Insanity          \n",
       "2   2020-10-12 03:48:35  763481578689425409   funny gal                  \n",
       "3   2020-10-12 03:48:34  874258472            Jonathan Greenberg         \n",
       "4   2020-10-12 03:48:34  1293187865820540928  conservative_Q             \n",
       "5   2020-10-12 03:48:34  871448794499620868   Palpatine‚Äôs Crappy Clone   \n",
       "6   2020-10-12 03:48:33  2364145870           JB Smith                   \n",
       "7   2020-10-12 03:48:33  237803067            VOTE NOVEMBER 3, 2020      \n",
       "8   2020-10-12 03:48:33  1046555609942896640  ayana_unwala               \n",
       "9   2020-10-12 03:48:32  829383893555912704   Rack Anne Ruin             \n",
       "10  2020-10-12 03:48:32  33474695             WendowsLane                \n",
       "11  2020-10-12 03:48:32  2852249984           Dr. Wylie Rogers, MD       \n",
       "12  2020-10-12 03:48:31  48794154             Thomas Paine               \n",
       "13  2020-10-12 03:48:31  1362703062           Mark Rathjen               \n",
       "14  2020-10-12 03:48:31  861896869            J.J                        \n",
       "15  2020-10-12 03:48:30  248725465            Melissa                    \n",
       "16  2020-10-12 03:48:30  1217028908           Julie                      \n",
       "17  2020-10-12 03:48:30  508170925            Jorge                      \n",
       "18  2020-10-12 03:48:30  626612323            david s                    \n",
       "19  2020-10-12 03:48:29  823950275253780480   Crystal                    \n",
       "20  2020-10-12 03:48:29  23316834             Amy                        \n",
       "21  2020-10-12 03:48:29  121603701            PSD                        \n",
       "22  2020-10-12 03:48:29  1177006826208014337  beatriz arturo             \n",
       "23  2020-10-12 03:48:29  190324301            JB                         \n",
       "24  2020-10-12 03:48:28  3355904255           James Carter               \n",
       "25  2020-10-12 03:48:28  28095959             j smith                    \n",
       "26  2020-10-12 03:48:28  31291327             #VOTE‚ö°Melissa‚ö° #VOTE       \n",
       "27  2020-10-12 03:48:28  1226159257428643841  Denise                     \n",
       "28  2020-10-12 03:48:28  990037494            sam funk üåà                 \n",
       "29  2020-10-12 03:48:27  49150104             üá∫üá∏ üáÆüá™ üçÄReene üáÆüáπ üç∑üá∫üá∏        \n",
       "\n",
       "               tweet_id  \\\n",
       "0                         \n",
       "0   1315499580344197121   \n",
       "1   1315499579400425477   \n",
       "2   1315499579027206149   \n",
       "3   1315499576082661381   \n",
       "4   1315499575587860480   \n",
       "5   1315499574946136064   \n",
       "6   1315499574082011136   \n",
       "7   1315499573574500352   \n",
       "8   1315499571792015361   \n",
       "9   1315499570189619200   \n",
       "10  1315499568897892354   \n",
       "11  1315499568352702464   \n",
       "12  1315499565437444098   \n",
       "13  1315499563390623744   \n",
       "14  1315499562740535297   \n",
       "15  1315499561335562240   \n",
       "16  1315499559636799491   \n",
       "17  1315499559565643776   \n",
       "18  1315499558646939650   \n",
       "19  1315499556621242369   \n",
       "20  1315499555245391872   \n",
       "21  1315499554414813185   \n",
       "22  1315499554226294785   \n",
       "23  1315499553878151169   \n",
       "24  1315499553320304640   \n",
       "25  1315499552959496192   \n",
       "26  1315499551336218626   \n",
       "27  1315499550338174976   \n",
       "28  1315499550141014016   \n",
       "29  1315499548735942656   \n",
       "\n",
       "                                                                                                                                                                            tweet  \\\n",
       "0                                                                                                                                                                                   \n",
       "0   @GadSaad @JoeBiden Most of the country simply sees Biden as a ‚Äúnot Trump‚Äù vote. This election is basically trump ru‚Ä¶ https://t.co/Xf16hWoQ5M                                    \n",
       "1   @Blue_Texas2020 @BryanRobinAR Bush won‚Äôt endorse Biden. Trump endorsed Jeb‚Äôs son for TX land commissioner and he wo‚Ä¶ https://t.co/3tKd114KyZ                                    \n",
       "2   RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶                                     \n",
       "3   @JimCarrey Why the Barrett confirm rush?\\r\\n \\r\\nHear Trump announce his plan to steal vote count with newly packed Supre‚Ä¶ https://t.co/wcazsPDSSi                              \n",
       "4   @DavidJHarrisJr You know it‚Äôs extremely hard to believe these polls when you see so much hacking for Trump at his g‚Ä¶ https://t.co/96mO83rhHg                                    \n",
       "5   RT @MrsRabbitResist: Trump supporters threatening to leave America if Biden wins is probably the best thing I‚Äôve heard all year. If any of‚Ä¶                                     \n",
       "6   @FernandoAmandi Castro-Chavistas overwhelmingly support Biden. Those who fled Castro-Chavistas overwhelmingly suppo‚Ä¶ https://t.co/I9hYO1XLhW                                    \n",
       "7   RT @Politics_Polls: GEORGIA\\r\\nBiden 47% (+1)\\r\\nTrump 46%\\r\\n.\\r\\n#GAsen:\\r\\nOssoff (D) 44% (+1)\\r\\nPerdue (R-inc) 43%\\r\\nHazel (L) 4%\\r\\n.\\r\\n#GAsen Special:\\r\\nWarnoc‚Ä¶      \n",
       "8   RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶                                    \n",
       "9   RT @acnewsitics: Hunter Biden was Chairman of the World Food Program,... which just won the Nobel Peace Prize.\\r\\n\\r\\nEric Trump, Donald J Trump‚Ä¶                               \n",
       "10  RT @Blue_Texas2020: Early voting begins in Texas in just 36 hours. My crystal balls says George Bush will endorse Biden within those 36 hou‚Ä¶                                    \n",
       "11  @BlackElleWoods As a man of color, trump can literally GFH. Biden/Harris 2020 https://t.co/1Cmu0JCrmp                                                                           \n",
       "12  RT @BenjaminCook: The #Vikings had more than a 95% win probability tonight before losing to the #Seahawks¬†. \\r\\n\\r\\nMost models put a VP Biden wi‚Ä¶                              \n",
       "13  RT @djrothkopf: It's happening again. It's just as bad. There are even more Republicans working to enable the Russians to help Trump this t‚Ä¶                                    \n",
       "14  RT @proxcee: If Biden is up in the polls and is supposed to beat Trump by a landslide...\\r\\n\\r\\nThey're gonna have to prove it to me on election‚Ä¶                               \n",
       "15  RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶                                    \n",
       "16  RT @SethAbramson: Actually, Trump publicly colluded with China and we all saw it on TV and then Bolton wrote about Trump colluding with Chi‚Ä¶                                    \n",
       "17  RT @WalshFreedom: I‚Äôm much, much, much more bothered by Trump‚Äôs undermining the legitimacy of our elections and his refusal to say he‚Äôll ac‚Ä¶                                    \n",
       "18  RT @nowthisnews: Biden on Trump: ‚ÄòAmerica deserves a president who understands what the American people are going through, who sees who you‚Ä¶                                    \n",
       "19  RT @RWTrollPatrol: Matt has always been a reliable republican voter but he can't stand Donald Trump and this American is voting for Joe Bid‚Ä¶                                    \n",
       "20  RT @djrothkopf: It's happening again. It's just as bad. There are even more Republicans working to enable the Russians to help Trump this t‚Ä¶                                    \n",
       "21  RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶                                     \n",
       "22  RT @EzraDrissman: Every Jewish voter should be very, very concerned. This is the man that singled out Jewish zip codes in NYC for lockdown.‚Ä¶                                    \n",
       "23  RT @nytimes: Jaime Harrison, the Democrat challenging Senator Lindsey Graham of South Carolina, raised an astonishing $57 million in the th‚Ä¶                                    \n",
       "24  RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶                                    \n",
       "25  RT @C_3C_3: The polls are as wrong as the they were in 2016.\\r\\n\\r\\nLook around you.\\r\\n\\r\\nThere are Latino Trump Car parades with 30K cars, 1000‚Äôs of‚Ä¶                        \n",
       "26  RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶                                     \n",
       "27  RT @BlueTidalWave: @LVaddict618 @itsJeffTiedrich That sucks!  I‚Äôm so sorry! üíô\\r\\nLet‚Äôs get rid of Trump the super~spreader!  \\r\\nüá∫üá∏üíôBiden~Harris2‚Ä¶                              \n",
       "28  RT @olivialovelier: Greta Thunberg endorsing biden isn't her selling out it's her being aware of trump's relationship with climate science,‚Ä¶                                    \n",
       "29  RT @Decimus581: üá∫üá∏DECIMUSüá∫üá∏\\r\\n@Staycee0322\\r\\n@TJF_0214\\r\\n@cruisejack4\\r\\n@Toddscrypto\\r\\n@Tplori1\\r\\n@mw4nc\\r\\n@MyxaZZ\\r\\n@Decimus581 \\r\\n@gladiatortcos \\r\\n@JoeT_2\\r\\n@‚Ä¶   \n",
       "\n",
       "   Sentiment  \n",
       "0             \n",
       "0   Positive  \n",
       "1   Positive  \n",
       "2   Positive  \n",
       "3   Negative  \n",
       "4   Negative  \n",
       "5   Negative  \n",
       "6   Positive  \n",
       "7   Positive  \n",
       "8   Positive  \n",
       "9   Positive  \n",
       "10  Positive  \n",
       "11  Positive  \n",
       "12  Negative  \n",
       "13  Negative  \n",
       "14  Positive  \n",
       "15  Positive  \n",
       "16  Positive  \n",
       "17  Negative  \n",
       "18  Negative  \n",
       "19  Positive  \n",
       "20  Negative  \n",
       "21  Positive  \n",
       "22  Negative  \n",
       "23  Negative  \n",
       "24  Positive  \n",
       "25  Negative  \n",
       "26  Positive  \n",
       "27  Negative  \n",
       "28  Negative  \n",
       "29  Positive  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>user_id</th>\n      <th>user_name</th>\n      <th>tweet_id</th>\n      <th>tweet</th>\n      <th>Sentiment</th>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>1220170588267958272</td>\n      <td>TedMazerDS</td>\n      <td>1315499580344197121</td>\n      <td>@GadSaad @JoeBiden Most of the country simply sees Biden as a ‚Äúnot Trump‚Äù vote. This election is basically trump ru‚Ä¶ https://t.co/Xf16hWoQ5M</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>587957065</td>\n      <td>Stop the Insanity</td>\n      <td>1315499579400425477</td>\n      <td>@Blue_Texas2020 @BryanRobinAR Bush won‚Äôt endorse Biden. Trump endorsed Jeb‚Äôs son for TX land commissioner and he wo‚Ä¶ https://t.co/3tKd114KyZ</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-10-12 03:48:35</td>\n      <td>763481578689425409</td>\n      <td>funny gal</td>\n      <td>1315499579027206149</td>\n      <td>RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-10-12 03:48:34</td>\n      <td>874258472</td>\n      <td>Jonathan Greenberg</td>\n      <td>1315499576082661381</td>\n      <td>@JimCarrey Why the Barrett confirm rush?\\r\\n \\r\\nHear Trump announce his plan to steal vote count with newly packed Supre‚Ä¶ https://t.co/wcazsPDSSi</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-10-12 03:48:34</td>\n      <td>1293187865820540928</td>\n      <td>conservative_Q</td>\n      <td>1315499575587860480</td>\n      <td>@DavidJHarrisJr You know it‚Äôs extremely hard to believe these polls when you see so much hacking for Trump at his g‚Ä¶ https://t.co/96mO83rhHg</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2020-10-12 03:48:34</td>\n      <td>871448794499620868</td>\n      <td>Palpatine‚Äôs Crappy Clone</td>\n      <td>1315499574946136064</td>\n      <td>RT @MrsRabbitResist: Trump supporters threatening to leave America if Biden wins is probably the best thing I‚Äôve heard all year. If any of‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2020-10-12 03:48:33</td>\n      <td>2364145870</td>\n      <td>JB Smith</td>\n      <td>1315499574082011136</td>\n      <td>@FernandoAmandi Castro-Chavistas overwhelmingly support Biden. Those who fled Castro-Chavistas overwhelmingly suppo‚Ä¶ https://t.co/I9hYO1XLhW</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2020-10-12 03:48:33</td>\n      <td>237803067</td>\n      <td>VOTE NOVEMBER 3, 2020</td>\n      <td>1315499573574500352</td>\n      <td>RT @Politics_Polls: GEORGIA\\r\\nBiden 47% (+1)\\r\\nTrump 46%\\r\\n.\\r\\n#GAsen:\\r\\nOssoff (D) 44% (+1)\\r\\nPerdue (R-inc) 43%\\r\\nHazel (L) 4%\\r\\n.\\r\\n#GAsen Special:\\r\\nWarnoc‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2020-10-12 03:48:33</td>\n      <td>1046555609942896640</td>\n      <td>ayana_unwala</td>\n      <td>1315499571792015361</td>\n      <td>RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2020-10-12 03:48:32</td>\n      <td>829383893555912704</td>\n      <td>Rack Anne Ruin</td>\n      <td>1315499570189619200</td>\n      <td>RT @acnewsitics: Hunter Biden was Chairman of the World Food Program,... which just won the Nobel Peace Prize.\\r\\n\\r\\nEric Trump, Donald J Trump‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2020-10-12 03:48:32</td>\n      <td>33474695</td>\n      <td>WendowsLane</td>\n      <td>1315499568897892354</td>\n      <td>RT @Blue_Texas2020: Early voting begins in Texas in just 36 hours. My crystal balls says George Bush will endorse Biden within those 36 hou‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2020-10-12 03:48:32</td>\n      <td>2852249984</td>\n      <td>Dr. Wylie Rogers, MD</td>\n      <td>1315499568352702464</td>\n      <td>@BlackElleWoods As a man of color, trump can literally GFH. Biden/Harris 2020 https://t.co/1Cmu0JCrmp</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2020-10-12 03:48:31</td>\n      <td>48794154</td>\n      <td>Thomas Paine</td>\n      <td>1315499565437444098</td>\n      <td>RT @BenjaminCook: The #Vikings had more than a 95% win probability tonight before losing to the #Seahawks¬†. \\r\\n\\r\\nMost models put a VP Biden wi‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2020-10-12 03:48:31</td>\n      <td>1362703062</td>\n      <td>Mark Rathjen</td>\n      <td>1315499563390623744</td>\n      <td>RT @djrothkopf: It's happening again. It's just as bad. There are even more Republicans working to enable the Russians to help Trump this t‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2020-10-12 03:48:31</td>\n      <td>861896869</td>\n      <td>J.J</td>\n      <td>1315499562740535297</td>\n      <td>RT @proxcee: If Biden is up in the polls and is supposed to beat Trump by a landslide...\\r\\n\\r\\nThey're gonna have to prove it to me on election‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2020-10-12 03:48:30</td>\n      <td>248725465</td>\n      <td>Melissa</td>\n      <td>1315499561335562240</td>\n      <td>RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2020-10-12 03:48:30</td>\n      <td>1217028908</td>\n      <td>Julie</td>\n      <td>1315499559636799491</td>\n      <td>RT @SethAbramson: Actually, Trump publicly colluded with China and we all saw it on TV and then Bolton wrote about Trump colluding with Chi‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2020-10-12 03:48:30</td>\n      <td>508170925</td>\n      <td>Jorge</td>\n      <td>1315499559565643776</td>\n      <td>RT @WalshFreedom: I‚Äôm much, much, much more bothered by Trump‚Äôs undermining the legitimacy of our elections and his refusal to say he‚Äôll ac‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2020-10-12 03:48:30</td>\n      <td>626612323</td>\n      <td>david s</td>\n      <td>1315499558646939650</td>\n      <td>RT @nowthisnews: Biden on Trump: ‚ÄòAmerica deserves a president who understands what the American people are going through, who sees who you‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2020-10-12 03:48:29</td>\n      <td>823950275253780480</td>\n      <td>Crystal</td>\n      <td>1315499556621242369</td>\n      <td>RT @RWTrollPatrol: Matt has always been a reliable republican voter but he can't stand Donald Trump and this American is voting for Joe Bid‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2020-10-12 03:48:29</td>\n      <td>23316834</td>\n      <td>Amy</td>\n      <td>1315499555245391872</td>\n      <td>RT @djrothkopf: It's happening again. It's just as bad. There are even more Republicans working to enable the Russians to help Trump this t‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2020-10-12 03:48:29</td>\n      <td>121603701</td>\n      <td>PSD</td>\n      <td>1315499554414813185</td>\n      <td>RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2020-10-12 03:48:29</td>\n      <td>1177006826208014337</td>\n      <td>beatriz arturo</td>\n      <td>1315499554226294785</td>\n      <td>RT @EzraDrissman: Every Jewish voter should be very, very concerned. This is the man that singled out Jewish zip codes in NYC for lockdown.‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2020-10-12 03:48:29</td>\n      <td>190324301</td>\n      <td>JB</td>\n      <td>1315499553878151169</td>\n      <td>RT @nytimes: Jaime Harrison, the Democrat challenging Senator Lindsey Graham of South Carolina, raised an astonishing $57 million in the th‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2020-10-12 03:48:28</td>\n      <td>3355904255</td>\n      <td>James Carter</td>\n      <td>1315499553320304640</td>\n      <td>RT @HelenKennedy: Joe Biden‚Äôs most recent endorsements: Cindy McCain, Scientific American, Greta Thunberg, the New England Journal of Medic‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2020-10-12 03:48:28</td>\n      <td>28095959</td>\n      <td>j smith</td>\n      <td>1315499552959496192</td>\n      <td>RT @C_3C_3: The polls are as wrong as the they were in 2016.\\r\\n\\r\\nLook around you.\\r\\n\\r\\nThere are Latino Trump Car parades with 30K cars, 1000‚Äôs of‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2020-10-12 03:48:28</td>\n      <td>31291327</td>\n      <td>#VOTE‚ö°Melissa‚ö° #VOTE</td>\n      <td>1315499551336218626</td>\n      <td>RT @JYSexton: It‚Äôs kind of amazing to watch people hound Biden over Court packing, a completely legal thing the system allows, while Trump‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2020-10-12 03:48:28</td>\n      <td>1226159257428643841</td>\n      <td>Denise</td>\n      <td>1315499550338174976</td>\n      <td>RT @BlueTidalWave: @LVaddict618 @itsJeffTiedrich That sucks!  I‚Äôm so sorry! üíô\\r\\nLet‚Äôs get rid of Trump the super~spreader!  \\r\\nüá∫üá∏üíôBiden~Harris2‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2020-10-12 03:48:28</td>\n      <td>990037494</td>\n      <td>sam funk üåà</td>\n      <td>1315499550141014016</td>\n      <td>RT @olivialovelier: Greta Thunberg endorsing biden isn't her selling out it's her being aware of trump's relationship with climate science,‚Ä¶</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2020-10-12 03:48:27</td>\n      <td>49150104</td>\n      <td>üá∫üá∏ üáÆüá™ üçÄReene üáÆüáπ üç∑üá∫üá∏</td>\n      <td>1315499548735942656</td>\n      <td>RT @Decimus581: üá∫üá∏DECIMUSüá∫üá∏\\r\\n@Staycee0322\\r\\n@TJF_0214\\r\\n@cruisejack4\\r\\n@Toddscrypto\\r\\n@Tplori1\\r\\n@mw4nc\\r\\n@MyxaZZ\\r\\n@Decimus581 \\r\\n@gladiatortcos \\r\\n@JoeT_2\\r\\n@‚Ä¶</td>\n      <td>Positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent = df[['tweet', 'Sentiment']].copy()\n",
    "\n",
    "tweet_sent.to_csv('tweet_sent.csv')"
   ]
  }
 ]
}